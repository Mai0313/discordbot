from typing import Any
import contextlib

from openai import AsyncStream
import logfire
import nextcord
from nextcord import Locale, Interaction, SlashOption
from nextcord.ext import commands
from openai.types.chat import ChatCompletionChunk

from discordbot.sdk.llm import LLMSDK

available_models = ["openrouter/x-ai/grok-4.1-fast"]
MODEL_CHOICES = {"grok-4.1-fast": "openrouter/x-ai/grok-4.1-fast"}
DEFAULT_MODEL = available_models[0]


class ReplyGeneratorCogs(commands.Cog):
    def __init__(self, bot: commands.Bot) -> None:
        """Initialize the ReplyGeneratorCogs.

        Args:
            bot (commands.Bot): The bot instance.
        """
        self.bot = bot
        # å„²å­˜æ¯å€‹ç”¨æˆ¶çš„å°è©±ç´€éŒ„
        # key: user_id, value: list of message dicts
        self.user_memory: dict[int, list[dict[str, Any]]] = {}

    async def _get_attachment_list(
        self, messages: list[nextcord.Message] | None = None
    ) -> list[str]:
        """Retrieve all attachments from a message.

        This function extracts image attachment URLs, embed descriptions, and converts sticker images to base64 encoded strings. If the message is None, an empty list is returned.

        Args:
            messages (Optional[list[nextcord.Message]]): The message from which to extract attachments.

        Returns:
            List[str]: A list containing the attachment URLs and base64 encoded sticker images.
        """
        if messages is None:
            messages = []
        attachments: list[str] = []
        for message in messages:
            if message.attachments:
                _attach = [attachment.url for attachment in message.attachments if attachment.url]
                if _attach:
                    attachments.extend(_attach)
            if message.embeds:
                _attach = [embed.description for embed in message.embeds if embed.description]
                if _attach:
                    attachments.extend(_attach)
            if message.stickers:
                _attach = [sticker.url for sticker in message.stickers]
                if _attach:
                    attachments.extend(_attach)
        return attachments

    @nextcord.slash_command(
        name="oai",
        description="I can reply from hints, search the web.",
        name_localizations={Locale.zh_TW: "ç”Ÿæˆ", Locale.ja: "ç”Ÿæˆ"},
        description_localizations={
            Locale.zh_TW: "æˆ‘å¯ä»¥å›žç­”å•é¡Œ, ä¸Šç¶²æœå°‹",
            Locale.ja: "æç¤ºã«åŸºã¥ã„ã¦è¿”ç­”ã‚’ç”Ÿæˆã—ã€æ¤œç´¢ã‚‚ã§ãã¾ã™ã€‚",
        },
        dm_permission=True,
        nsfw=False,
    )
    async def oai(
        self,
        interaction: Interaction,
        prompt: str = SlashOption(
            description="Enter your prompt.",
            description_localizations={
                Locale.zh_TW: "è«‹è¼¸å…¥æç¤ºè©žã€‚",
                Locale.ja: "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
            },
        ),
        model: str = SlashOption(
            description="Choose a model (default: GPT-5).",
            description_localizations={
                Locale.zh_TW: "é¸æ“‡æ¨¡åž‹ (é è¨­ç‚º GPT-5)",
                Locale.ja: "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠžã—ã¦ãã ã•ã„ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ GPT-5ï¼‰",
            },
            choices=MODEL_CHOICES,
            required=False,
            default=available_models[0],
        ),
        image: nextcord.Attachment | None = SlashOption(  # noqa: B008
            description="(Optional) Upload an image.",
            description_localizations={
                Locale.zh_TW: "ï¼ˆå¯é¸ï¼‰ä¸Šå‚³ä¸€å¼µåœ–ç‰‡ã€‚",
                Locale.ja: "ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
            },
            required=False,
        ),
    ) -> None:
        """Generate a reply based on the user's prompt.

        Args:
            interaction (Interaction): The interaction object for the command.
            prompt (str): The prompt text provided by the user.
            model (str): The selected model, defaults to "gpt-5" if not specified.
            image (Optional[nextcord.Attachment]): An optional image attachment uploaded by the user.
        """
        await interaction.response.defer()
        attachments = []
        if image:
            attachments.append(image.url)

        # åˆå§‹ç‹€æ…‹è¨Šæ¯
        await interaction.followup.send(content="ðŸ¤” æ€è€ƒä¸­...")

        try:
            llm_sdk = LLMSDK(model=model)
            # ä½¿ç”¨ completion content æ ¼å¼ (ChatCompletion)
            content = await llm_sdk.prepare_completion_content(
                prompt=prompt, attachments=attachments
            )
            content = f"You are not allowed to use Simplified Chinese in your response.\n{content}"

            user_id = interaction.user.id
            if user_id not in self.user_memory:
                self.user_memory[user_id] = []

            # å°‡ç”¨æˆ¶è¨Šæ¯åŠ å…¥è¨˜æ†¶
            self.user_memory[user_id].append({"role": "user", "content": content})

            try:
                stream = await llm_sdk.client.chat.completions.create(
                    model=model, messages=self.user_memory[user_id], stream=True
                )
            except Exception as e:
                # è‹¥ç™¼ç”ŸéŒ¯èª¤ï¼Œå¯èƒ½æ˜¯ content filter æˆ–å…¶ä»–å•é¡Œï¼Œä¸æ¸…é™¤è¨˜æ†¶ä½†å ±éŒ¯
                # æˆ–æ˜¯å¦‚æžœ memory å¤ªé•·å°Žè‡´ context length exceededï¼Œå¯èƒ½éœ€è¦æ¸…ç†
                # é€™è£¡ç°¡å–®å ±éŒ¯
                logfire.error("Error creating chat completion", _exc_info=True)
                raise e

            response_text = await self._handle_streaming_response(
                interaction=interaction, stream=stream, update_per_words=10
            )

            # å°‡ AI å›žæ‡‰åŠ å…¥è¨˜æ†¶
            if response_text:
                self.user_memory[user_id].append({"role": "assistant", "content": response_text})

        except Exception as e:
            await interaction.edit_original_message(
                content=f"{interaction.user.mention}\nâŒ éŒ¯èª¤:\n{e}"
            )
            logfire.error("Error in oai", _exc_info=True)

    async def _handle_streaming_response(
        self,
        interaction: Interaction,
        stream: AsyncStream[ChatCompletionChunk],
        update_per_words: int = 10,
    ) -> str:
        """è™•ç† streaming å›žæ‡‰ï¼Œæ¯ 10 å€‹å­—æ›´æ–°ä¸€æ¬¡è¨Šæ¯ã€‚

        Returns:
            str: å®Œæ•´çš„ç”Ÿæˆæ–‡å­—
        """
        accumulated_text = ""
        char_count = 0

        async for chunk in stream:
            if not chunk.choices:
                continue

            delta = chunk.choices[0].delta
            if delta.content:
                accumulated_text += delta.content
                char_count += len(delta.content)

                # æ¯ X å€‹å­—æ›´æ–°ä¸€æ¬¡è¨Šæ¯
                if char_count >= update_per_words:
                    try:
                        await interaction.edit_original_message(
                            content=f"{interaction.user.mention}\n{accumulated_text}"
                        )
                        char_count = 0
                    except nextcord.errors.NotFound:
                        # è¨Šæ¯å¯èƒ½è¢«åˆªé™¤
                        break
                    except Exception as e:
                        logfire.warning(f"Failed to update message: {e}")

        # æœ€çµ‚æ›´æ–°ç¢ºä¿é¡¯ç¤ºå®Œæ•´è¨Šæ¯
        with contextlib.suppress(Exception):
            await interaction.edit_original_message(
                content=f"{interaction.user.mention}\n{accumulated_text}"
            )

        return accumulated_text

    @nextcord.slash_command(
        name="clear_memory",
        description="Clear your conversation memory with the bot.",
        name_localizations={Locale.zh_TW: "æ¸…é™¤è¨˜æ†¶", Locale.ja: "ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢"},
        description_localizations={
            Locale.zh_TW: "æ¸…é™¤ä½ èˆ‡æ©Ÿå™¨äººçš„å°è©±è¨˜æ†¶ã€‚",
            Locale.ja: "ãƒœãƒƒãƒˆã¨ã®ä¼šè©±ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚",
        },
        dm_permission=True,
        nsfw=False,
    )
    async def clear_memory(self, interaction: Interaction) -> None:
        """æ¸…é™¤ç”¨æˆ¶çš„å°è©±è¨˜æ†¶ã€‚

        Args:
            interaction (Interaction): The interaction object for the command.
        """
        user_id = interaction.user.id
        had_memory = self.user_memory.pop(user_id, None) is not None

        if had_memory:
            await interaction.response.send_message(
                content="å°è©±è¨˜æ†¶å·²æ¸…é™¤! ä¸‹æ¬¡å°è©±å°‡é‡æ–°é–‹å§‹ã€‚", ephemeral=True
            )
        else:
            await interaction.response.send_message(
                content="ä½ ç›®å‰æ²’æœ‰å°è©±è¨˜æ†¶éœ€è¦æ¸…é™¤ã€‚", ephemeral=True
            )

    @commands.Cog.listener()
    async def on_message(self, message: nextcord.Message) -> None:
        """Handle messages that mention the bot.

        This listener allows users to chat with the bot by mentioning it,
        without needing to use slash commands.

        Args:
            message (nextcord.Message): The message object.
        """
        # Ignore messages from bots (including self)
        if message.author.bot:
            return

        # Check if bot is mentioned
        if self.bot.user not in message.mentions:
            return

        # Extract message content without mentions
        content = message.content
        for mention in message.mentions:
            content = content.replace(f"<@{mention.id}>", "").replace(f"<@!{mention.id}>", "")
        content = content.strip()

        # If content is empty or only whitespace, reply with "?"
        if not content:
            await message.reply("?")
            return

        # Get attachments from the message
        attachments = await self._get_attachment_list([message])

        # Start typing indicator
        async with message.channel.typing():
            try:
                llm_sdk = LLMSDK(model=DEFAULT_MODEL)
                # Prepare completion content
                completion_content = await llm_sdk.prepare_completion_content(
                    prompt=content, attachments=attachments
                )
                completion_content = f"You are not allowed to use Simplified Chinese in your response.\n{completion_content}"

                user_id = message.author.id
                if user_id not in self.user_memory:
                    self.user_memory[user_id] = []

                # Add user message to memory
                self.user_memory[user_id].append({"role": "user", "content": completion_content})

                try:
                    stream = await llm_sdk.client.chat.completions.create(
                        model=DEFAULT_MODEL, messages=self.user_memory[user_id], stream=True
                    )
                except Exception as e:
                    logfire.error("Error creating chat completion for mention", _exc_info=True)
                    await message.reply(f"âŒ éŒ¯èª¤: {e}")
                    return

                # Send initial thinking message
                reply_message = await message.reply("ðŸ¤” æ€è€ƒä¸­...")

                # Handle streaming response
                response_text = await self._handle_streaming_response_for_message(
                    reply_message=reply_message, stream=stream, update_per_words=10
                )

                # Add AI response to memory
                if response_text:
                    self.user_memory[user_id].append({
                        "role": "assistant",
                        "content": response_text,
                    })

            except Exception as e:
                logfire.error("Error in on_message mention handler", _exc_info=True)
                await message.reply(f"âŒ éŒ¯èª¤: {e}")

    async def _handle_streaming_response_for_message(
        self,
        reply_message: nextcord.Message,
        stream: AsyncStream[ChatCompletionChunk],
        update_per_words: int = 10,
    ) -> str:
        """Handle streaming response for regular message replies.

        Args:
            reply_message (nextcord.Message): The message to edit with streaming content.
            stream (AsyncStream[ChatCompletionChunk]): The streaming response from LLM.
            update_per_words (int): Update message every N characters.

        Returns:
            str: The complete generated text.
        """
        accumulated_text = ""
        char_count = 0

        async for chunk in stream:
            if not chunk.choices:
                continue

            delta = chunk.choices[0].delta
            if delta.content:
                accumulated_text += delta.content
                char_count += len(delta.content)

                # Update message every X characters
                if char_count >= update_per_words:
                    try:
                        await reply_message.edit(content=accumulated_text)
                        char_count = 0
                    except nextcord.errors.NotFound:
                        # Message might have been deleted
                        break
                    except Exception as e:
                        logfire.warning(f"Failed to update message: {e}")

        # Final update to ensure complete message is displayed
        with contextlib.suppress(Exception):
            await reply_message.edit(content=accumulated_text)

        return accumulated_text


async def setup(bot: commands.Bot) -> None:
    """Register the reply generation cog with the bot.

    Args:
        bot (commands.Bot): The bot instance to which the cog will be added.
    """
    bot.add_cog(ReplyGeneratorCogs(bot), override=True)
