from typing import Any

from openai import BadRequestError
import logfire
import nextcord
from nextcord import Locale, Interaction, SlashOption
from nextcord.ext import commands

from discordbot.sdk.llm import LLMSDK

available_models = ["openai/gpt-5-mini", "openai/gpt-5-nano", "claude-3-5-haiku-20241022"]
MODEL_CHOICES = {available_model: available_model for available_model in available_models}


class ReplyGeneratorCogs(commands.Cog):
    def __init__(self, bot: commands.Bot) -> None:
        """Initialize the ReplyGeneratorCogs.

        Args:
            bot (commands.Bot): The bot instance.
        """
        self.bot = bot
        # å„²å­˜æ¯å€‹ç”¨æˆ¶çš„ä¸Šä¸€å€‹ response IDï¼Œç”¨æ–¼å°è©±è¨˜æ†¶
        self.user_last_response_id: dict[int, str] = {}

    async def _get_attachment_list(
        self, messages: list[nextcord.Message] | None = None
    ) -> list[str]:
        """Retrieve all attachments from a message.

        This function extracts image attachment URLs, embed descriptions, and converts sticker images to base64 encoded strings. If the message is None, an empty list is returned.

        Args:
            messages (Optional[list[nextcord.Message]]): The message from which to extract attachments.

        Returns:
            List[str]: A list containing the attachment URLs and base64 encoded sticker images.
        """
        if messages is None:
            messages = []
        attachments: list[str] = []
        for message in messages:
            if message.attachments:
                _attach = [attachment.url for attachment in message.attachments if attachment.url]
                if _attach:
                    attachments.extend(_attach)
            if message.embeds:
                _attach = [embed.description for embed in message.embeds if embed.description]
                if _attach:
                    attachments.extend(_attach)
            if message.stickers:
                _attach = [sticker.url for sticker in message.stickers]
                if _attach:
                    attachments.extend(_attach)
        return attachments

    @nextcord.slash_command(
        name="oai",
        description="Generate a reply based on the given prompt.",
        name_localizations={Locale.zh_TW: "ç”Ÿæˆæ–‡å­—", Locale.ja: "ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"},
        description_localizations={
            Locale.zh_TW: "æ ¹æ“šæä¾›çš„æç¤ºç”Ÿæˆå›è¦†ã€‚",
            Locale.ja: "æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ã„ã¦å¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
        },
        dm_permission=True,
        nsfw=False,
    )
    async def oai(
        self,
        interaction: Interaction,
        prompt: str = SlashOption(
            description="Enter your prompt.",
            description_localizations={
                Locale.zh_TW: "è«‹è¼¸å…¥æç¤ºè©ã€‚",
                Locale.ja: "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
            },
        ),
        model: str = SlashOption(
            description="Choose a model (default: GPT-5).",
            description_localizations={
                Locale.zh_TW: "é¸æ“‡æ¨¡å‹ (é è¨­ç‚º GPT-5)",
                Locale.ja: "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ GPT-5ï¼‰",
            },
            choices=MODEL_CHOICES,
            required=False,
            default=available_models[0],
        ),
        image: nextcord.Attachment | None = SlashOption(  # noqa: B008
            description="(Optional) Upload an image.",
            description_localizations={
                Locale.zh_TW: "ï¼ˆå¯é¸ï¼‰ä¸Šå‚³ä¸€å¼µåœ–ç‰‡ã€‚",
                Locale.ja: "ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
            },
            required=False,
        ),
    ) -> None:
        """Generate a reply based on the user's prompt.

        If the model 'o1' is selected along with an image, an error message is returned since 'o1' does not support image input.
        The function can either generate a complete response or stream the response in real-time based on the stream parameter.

        Args:
            interaction (Interaction): The interaction object for the command.
            prompt (str): The prompt text provided by the user.
            model (str): The selected model, defaults to "gpt-5" if not specified.
            image (Optional[nextcord.Attachment]): An optional image attachment uploaded by the user.
        """
        await interaction.response.defer()
        attachments = []
        if model not in ["o1", "o1-mini"] and image:
            attachments.append(image.url)

        await interaction.followup.send(content="Thinking...")

        try:
            llm_sdk = LLMSDK(model=model)
            content = await llm_sdk.prepare_response_content(
                prompt=prompt, attachments=attachments
            )
            try:
                # ç²å–ç”¨æˆ¶çš„æœ€æ–° response ID
                previous_response_id = self.user_last_response_id.get(interaction.user.id, None)
                responses = await llm_sdk.client.responses.create(
                    model=model,
                    tools=[{"type": "web_search_preview"}, {"type": "image_generation"}],
                    input=[{"role": "user", "content": content}],
                    previous_response_id=previous_response_id,
                )
            except BadRequestError:
                # å¦‚æœ API å›å‚³éŒ¯èª¤ï¼ˆresponse ID ç„¡æ•ˆï¼‰ï¼Œæ¸…ç†è©²ç”¨æˆ¶è¨˜éŒ„ä¸¦é‡æ–°å˜—è©¦
                self.user_last_response_id.pop(interaction.user.id, None)
                responses = await llm_sdk.client.responses.create(
                    model=model,
                    tools=[{"type": "web_search_preview"}, {"type": "image_generation"}],
                    input=[{"role": "user", "content": content}],
                )

            # å„²å­˜æ–°çš„ response ID
            self.user_last_response_id[interaction.user.id] = responses.id

            # é™„ä¸Šã€Œé‡æ–°ç”Ÿæˆã€æŒ‰éˆ•
            view = OAIRegenerateView(
                cog=self,
                requester_id=interaction.user.id,
                user_id=interaction.user.id,
                model=model,
                content=content,
            )

            edit_kwargs: dict[str, Any] = {
                "content": f"{interaction.user.mention}\n{responses.output_text}",
                "view": view,
            }
            await interaction.edit_original_message(**edit_kwargs)

        except Exception as e:
            await interaction.edit_original_message(content=f"{e}")
            logfire.error("Error in oai", _exc_info=True)

    @nextcord.slash_command(
        name="clear_memory",
        description="Clear your conversation memory with the bot.",
        name_localizations={Locale.zh_TW: "æ¸…é™¤è¨˜æ†¶", Locale.ja: "ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢"},
        description_localizations={
            Locale.zh_TW: "æ¸…é™¤ä½ èˆ‡æ©Ÿå™¨äººçš„å°è©±è¨˜æ†¶ã€‚",
            Locale.ja: "ãƒœãƒƒãƒˆã¨ã®ä¼šè©±ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚",
        },
        dm_permission=True,
        nsfw=False,
    )
    async def clear_memory(self, interaction: Interaction) -> None:
        """æ¸…é™¤ç”¨æˆ¶çš„å°è©±è¨˜æ†¶ã€‚

        Args:
            interaction (Interaction): The interaction object for the command.
        """
        user_id = interaction.user.id
        had_memory = self.user_last_response_id.pop(user_id, None) is not None

        if had_memory:
            await interaction.response.send_message(
                content="å°è©±è¨˜æ†¶å·²æ¸…é™¤! ä¸‹æ¬¡å°è©±å°‡é‡æ–°é–‹å§‹ã€‚", ephemeral=True
            )
        else:
            await interaction.response.send_message(
                content="ä½ ç›®å‰æ²’æœ‰å°è©±è¨˜æ†¶éœ€è¦æ¸…é™¤ã€‚", ephemeral=True
            )


async def setup(bot: commands.Bot) -> None:
    """Register the reply generation cog with the bot.

    Args:
        bot (commands.Bot): The bot instance to which the cog will be added.
    """
    bot.add_cog(ReplyGeneratorCogs(bot), override=True)


class OAIRegenerateView(nextcord.ui.View):
    def __init__(
        self,
        cog: ReplyGeneratorCogs,
        requester_id: int,
        user_id: int,
        model: str,
        content: list[dict[str, Any]],
        timeout: float = 600,
    ) -> None:
        super().__init__(timeout=timeout)
        self.cog = cog
        self.requester_id = requester_id
        self.user_id = user_id
        self.model = model
        self.content = content

    @nextcord.ui.button(label="é‡æ–°ç”Ÿæˆ", emoji="ğŸ”", style=nextcord.ButtonStyle.blurple)
    async def regenerate(self, button: nextcord.ui.Button, interaction: Interaction) -> None:
        if interaction.user.id != self.requester_id:
            await interaction.response.send_message("æ­¤æŒ‰éˆ•åƒ…é™åŸè«‹æ±‚è€…ä½¿ç”¨ã€‚", ephemeral=True)
            return

        await interaction.response.defer()
        await interaction.response.edit_message(content="Thinking...", view=self)

        try:
            llm_sdk = LLMSDK(model=self.model)
            previous_id = self.cog.user_last_response_id.get(self.user_id, None)
            responses = await llm_sdk.client.responses.create(
                model=self.model,
                tools=[{"type": "web_search_preview"}],
                input=[{"role": "user", "content": self.content}],
                previous_response_id=previous_id,
            )

            # æ›´æ–°å°è©±æœ€æ–° IDï¼ˆä¾ä½œç”¨åŸŸï¼‰
            self.cog.user_last_response_id[self.user_id] = responses.id

            edit_kwargs: dict[str, Any] = {
                "content": f"{interaction.user.mention}\n{responses.output_text}",
                "view": self,
            }

            await interaction.followup.edit_message(
                message_id=interaction.message.id, **edit_kwargs
            )

        except Exception as e:
            await interaction.followup.send(content=f"é‡æ–°ç”Ÿæˆå¤±æ•—ï¼š{e}", ephemeral=True)
