from typing import Any
import contextlib

from openai import AsyncStream
import logfire
import nextcord
from nextcord import Locale, Interaction, SlashOption
from nextcord.ext import commands
from openai.types.chat import ChatCompletionChunk

from discordbot.sdk.llm import LLMSDK

available_models = ["openrouter/x-ai/grok-4.1-fast"]
MODEL_CHOICES = {"grok-4.1-fast": "openrouter/x-ai/grok-4.1-fast"}


class ReplyGeneratorCogs(commands.Cog):
    def __init__(self, bot: commands.Bot) -> None:
        """Initialize the ReplyGeneratorCogs.

        Args:
            bot (commands.Bot): The bot instance.
        """
        self.bot = bot
        # å„²å­˜æ¯å€‹ç”¨æˆ¶çš„å°è©±ç´€éŒ„
        # key: user_id, value: list of message dicts
        self.user_memory: dict[int, list[dict[str, Any]]] = {}

    async def _get_attachment_list(
        self, messages: list[nextcord.Message] | None = None
    ) -> list[str]:
        """Retrieve all attachments from a message.

        This function extracts image attachment URLs, embed descriptions, and converts sticker images to base64 encoded strings. If the message is None, an empty list is returned.

        Args:
            messages (Optional[list[nextcord.Message]]): The message from which to extract attachments.

        Returns:
            List[str]: A list containing the attachment URLs and base64 encoded sticker images.
        """
        if messages is None:
            messages = []
        attachments: list[str] = []
        for message in messages:
            if message.attachments:
                _attach = [attachment.url for attachment in message.attachments if attachment.url]
                if _attach:
                    attachments.extend(_attach)
            if message.embeds:
                _attach = [embed.description for embed in message.embeds if embed.description]
                if _attach:
                    attachments.extend(_attach)
            if message.stickers:
                _attach = [sticker.url for sticker in message.stickers]
                if _attach:
                    attachments.extend(_attach)
        return attachments

    @nextcord.slash_command(
        name="oai",
        description="I can reply from hints, search the web.",
        name_localizations={Locale.zh_TW: "ç”Ÿæˆ", Locale.ja: "ç”Ÿæˆ"},
        description_localizations={
            Locale.zh_TW: "æˆ‘å¯ä»¥å›žç­”å•é¡Œ, ä¸Šç¶²æœå°‹",
            Locale.ja: "æç¤ºã«åŸºã¥ã„ã¦è¿”ç­”ã‚’ç”Ÿæˆã—ã€æ¤œç´¢ã‚‚ã§ãã¾ã™ã€‚",
        },
        dm_permission=True,
        nsfw=False,
    )
    async def oai(
        self,
        interaction: Interaction,
        prompt: str = SlashOption(
            description="Enter your prompt.",
            description_localizations={
                Locale.zh_TW: "è«‹è¼¸å…¥æç¤ºè©žã€‚",
                Locale.ja: "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
            },
        ),
        model: str = SlashOption(
            description="Choose a model (default: GPT-5).",
            description_localizations={
                Locale.zh_TW: "é¸æ“‡æ¨¡åž‹ (é è¨­ç‚º GPT-5)",
                Locale.ja: "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠžã—ã¦ãã ã•ã„ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ GPT-5ï¼‰",
            },
            choices=MODEL_CHOICES,
            required=False,
            default=available_models[0],
        ),
        image: nextcord.Attachment | None = SlashOption(  # noqa: B008
            description="(Optional) Upload an image.",
            description_localizations={
                Locale.zh_TW: "ï¼ˆå¯é¸ï¼‰ä¸Šå‚³ä¸€å¼µåœ–ç‰‡ã€‚",
                Locale.ja: "ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
            },
            required=False,
        ),
    ) -> None:
        """Generate a reply based on the user's prompt.

        Args:
            interaction (Interaction): The interaction object for the command.
            prompt (str): The prompt text provided by the user.
            model (str): The selected model, defaults to "gpt-5" if not specified.
            image (Optional[nextcord.Attachment]): An optional image attachment uploaded by the user.
        """
        await interaction.response.defer()
        attachments = []
        if image:
            attachments.append(image.url)

        # åˆå§‹ç‹€æ…‹è¨Šæ¯
        await interaction.followup.send(content="ðŸ¤” æ€è€ƒä¸­...")

        try:
            llm_sdk = LLMSDK(model=model)
            # ä½¿ç”¨ completion content æ ¼å¼ (ChatCompletion)
            content = await llm_sdk.prepare_completion_content(
                prompt=prompt, attachments=attachments
            )
            content = f"You are not allowed to use Simplified Chinese in your response.\n{content}"

            user_id = interaction.user.id
            if user_id not in self.user_memory:
                self.user_memory[user_id] = []

            # å°‡ç”¨æˆ¶è¨Šæ¯åŠ å…¥è¨˜æ†¶
            self.user_memory[user_id].append({"role": "user", "content": content})

            try:
                stream = await llm_sdk.client.chat.completions.create(
                    model=model, messages=self.user_memory[user_id], stream=True
                )
            except Exception as e:
                # è‹¥ç™¼ç”ŸéŒ¯èª¤ï¼Œå¯èƒ½æ˜¯ content filter æˆ–å…¶ä»–å•é¡Œï¼Œä¸æ¸…é™¤è¨˜æ†¶ä½†å ±éŒ¯
                # æˆ–æ˜¯å¦‚æžœ memory å¤ªé•·å°Žè‡´ context length exceededï¼Œå¯èƒ½éœ€è¦æ¸…ç†
                # é€™è£¡ç°¡å–®å ±éŒ¯
                logfire.error("Error creating chat completion", _exc_info=True)
                raise e

            response_text = await self._handle_streaming_response(
                interaction=interaction, stream=stream, update_per_words=10
            )

            # å°‡ AI å›žæ‡‰åŠ å…¥è¨˜æ†¶
            if response_text:
                self.user_memory[user_id].append({"role": "assistant", "content": response_text})

        except Exception as e:
            await interaction.edit_original_message(
                content=f"{interaction.user.mention}\nâŒ éŒ¯èª¤:\n{e}"
            )
            logfire.error("Error in oai", _exc_info=True)

    async def _handle_streaming_response(
        self,
        interaction: Interaction,
        stream: AsyncStream[ChatCompletionChunk],
        update_per_words: int = 10,
    ) -> str:
        """è™•ç† streaming å›žæ‡‰ï¼Œæ¯ 10 å€‹å­—æ›´æ–°ä¸€æ¬¡è¨Šæ¯ã€‚

        Returns:
            str: å®Œæ•´çš„ç”Ÿæˆæ–‡å­—
        """
        accumulated_text = ""
        char_count = 0

        async for chunk in stream:
            if not chunk.choices:
                continue

            delta = chunk.choices[0].delta
            if delta.content:
                accumulated_text += delta.content
                char_count += len(delta.content)

                # æ¯ X å€‹å­—æ›´æ–°ä¸€æ¬¡è¨Šæ¯
                if char_count >= update_per_words:
                    try:
                        await interaction.edit_original_message(
                            content=f"{interaction.user.mention}\n{accumulated_text}"
                        )
                        char_count = 0
                    except nextcord.errors.NotFound:
                        # è¨Šæ¯å¯èƒ½è¢«åˆªé™¤
                        break
                    except Exception as e:
                        logfire.warning(f"Failed to update message: {e}")

        # æœ€çµ‚æ›´æ–°ç¢ºä¿é¡¯ç¤ºå®Œæ•´è¨Šæ¯
        with contextlib.suppress(Exception):
            await interaction.edit_original_message(
                content=f"{interaction.user.mention}\n{accumulated_text}"
            )

        return accumulated_text

    @nextcord.slash_command(
        name="clear_memory",
        description="Clear your conversation memory with the bot.",
        name_localizations={Locale.zh_TW: "æ¸…é™¤è¨˜æ†¶", Locale.ja: "ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢"},
        description_localizations={
            Locale.zh_TW: "æ¸…é™¤ä½ èˆ‡æ©Ÿå™¨äººçš„å°è©±è¨˜æ†¶ã€‚",
            Locale.ja: "ãƒœãƒƒãƒˆã¨ã®ä¼šè©±ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚",
        },
        dm_permission=True,
        nsfw=False,
    )
    async def clear_memory(self, interaction: Interaction) -> None:
        """æ¸…é™¤ç”¨æˆ¶çš„å°è©±è¨˜æ†¶ã€‚

        Args:
            interaction (Interaction): The interaction object for the command.
        """
        user_id = interaction.user.id
        had_memory = self.user_memory.pop(user_id, None) is not None

        if had_memory:
            await interaction.response.send_message(
                content="å°è©±è¨˜æ†¶å·²æ¸…é™¤! ä¸‹æ¬¡å°è©±å°‡é‡æ–°é–‹å§‹ã€‚", ephemeral=True
            )
        else:
            await interaction.response.send_message(
                content="ä½ ç›®å‰æ²’æœ‰å°è©±è¨˜æ†¶éœ€è¦æ¸…é™¤ã€‚", ephemeral=True
            )


async def setup(bot: commands.Bot) -> None:
    """Register the reply generation cog with the bot.

    Args:
        bot (commands.Bot): The bot instance to which the cog will be added.
    """
    bot.add_cog(ReplyGeneratorCogs(bot), override=True)
